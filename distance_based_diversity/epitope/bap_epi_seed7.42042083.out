Running BAP with embedding: catELMo_4_layers_1024 split: epitope seed: 7 fraction: 1.0
Loading embeddings from: data/pairs7_embeds.pkl
Total pairs: 10500
Embedding dim: 2048
================check Overlapping========================
number of overlapping tcrs:  10
number of overlapping epitopes:  0
train size: 8498 test size: 2002
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 2048)]       0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, 2048)]       0                                            
__________________________________________________________________________________________________
dense (Dense)                   (None, 2048)         4196352     input_1[0][0]                    
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 2048)         4196352     input_2[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 2048)         8192        dense[0][0]                      
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 2048)         8192        dense_1[0][0]                    
__________________________________________________________________________________________________
dropout (Dropout)               (None, 2048)         0           batch_normalization[0][0]        
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 2048)         0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
tf.nn.silu (TFOpLambda)         (None, 2048)         0           dropout[0][0]                    
__________________________________________________________________________________________________
tf.nn.silu_1 (TFOpLambda)       (None, 2048)         0           dropout_1[0][0]                  
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 4096)         0           tf.nn.silu[0][0]                 
                                                                 tf.nn.silu_1[0][0]               
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 1024)         4195328     concatenate[0][0]                
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 1024)         4096        dense_2[0][0]                    
__________________________________________________________________________________________________
dropout_2 (Dropout)             (None, 1024)         0           batch_normalization_2[0][0]      
__________________________________________________________________________________________________
tf.nn.silu_2 (TFOpLambda)       (None, 1024)         0           dropout_2[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 1)            1025        tf.nn.silu_2[0][0]               
==================================================================================================
Total params: 12,609,537
Trainable params: 12,599,297
Non-trainable params: 10,240
__________________________________________________________________________________________________
Epoch 1/50
60/60 - 5s - loss: 0.7641 - AUC: 0.6397 - accuracy: 0.6022 - val_loss: 0.6587 - val_AUC: 0.6875 - val_accuracy: 0.6153
Epoch 2/50
60/60 - 4s - loss: 0.6611 - AUC: 0.7154 - accuracy: 0.6529 - val_loss: 0.6437 - val_AUC: 0.6925 - val_accuracy: 0.6341
Epoch 3/50
60/60 - 4s - loss: 0.6233 - AUC: 0.7460 - accuracy: 0.6780 - val_loss: 0.6325 - val_AUC: 0.6931 - val_accuracy: 0.6482
Epoch 4/50
60/60 - 4s - loss: 0.5954 - AUC: 0.7665 - accuracy: 0.6908 - val_loss: 0.6128 - val_AUC: 0.7109 - val_accuracy: 0.6353
Epoch 5/50
60/60 - 4s - loss: 0.5764 - AUC: 0.7821 - accuracy: 0.7079 - val_loss: 0.6164 - val_AUC: 0.7058 - val_accuracy: 0.6294
Epoch 6/50
60/60 - 4s - loss: 0.5542 - AUC: 0.7992 - accuracy: 0.7218 - val_loss: 0.6115 - val_AUC: 0.7290 - val_accuracy: 0.6529
Epoch 7/50
60/60 - 4s - loss: 0.5329 - AUC: 0.8123 - accuracy: 0.7299 - val_loss: 0.6091 - val_AUC: 0.7237 - val_accuracy: 0.6529
Epoch 8/50
60/60 - 4s - loss: 0.5350 - AUC: 0.8102 - accuracy: 0.7255 - val_loss: 0.6071 - val_AUC: 0.7328 - val_accuracy: 0.6541
Epoch 9/50
60/60 - 4s - loss: 0.5052 - AUC: 0.8315 - accuracy: 0.7504 - val_loss: 0.6244 - val_AUC: 0.7365 - val_accuracy: 0.6741
Epoch 10/50
60/60 - 4s - loss: 0.4984 - AUC: 0.8363 - accuracy: 0.7518 - val_loss: 0.6092 - val_AUC: 0.7410 - val_accuracy: 0.6565
Epoch 11/50
60/60 - 4s - loss: 0.4767 - AUC: 0.8503 - accuracy: 0.7644 - val_loss: 0.6083 - val_AUC: 0.7360 - val_accuracy: 0.6635
Epoch 12/50
60/60 - 4s - loss: 0.4795 - AUC: 0.8481 - accuracy: 0.7623 - val_loss: 0.6193 - val_AUC: 0.7355 - val_accuracy: 0.6624
Epoch 13/50
60/60 - 4s - loss: 0.4595 - AUC: 0.8605 - accuracy: 0.7704 - val_loss: 0.6142 - val_AUC: 0.7284 - val_accuracy: 0.6553
Restoring model weights from the end of the best epoch.
Epoch 00013: early stopping
================Performance========================
catELMo_4_layers_1024_epitope_seed_7_fraction_1.0AUC: 0.6884174766292648
precision_recall_fscore_macro (0.6335206750296, 0.6333666333666333, 0.6332608574121104, None)
acc is 0.6333666333666333
precision1 is 0.6380558428128231
precision0 is 0.6289855072463768
recall1 is 0.6163836163836164
recall0 is 0.6503496503496503
f1macro is 0.6332608574121104
f1micro is 0.6333666333666333
